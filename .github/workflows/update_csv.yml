name: Atualizar CSV com Web Scraping

on:
  schedule:
    - cron: '0 3 * * *'  # Roda todos os dias às 03:00 UTC
  workflow_dispatch:  # Permite rodar manualmente

jobs:
  update_csv:
    runs-on: ubuntu-latest

    steps:
      - name: Clonar o repositório
        uses: actions/checkout@v4

      - name: Instalar o Chromium e o ChromeDriver
        run: |
          sudo apt-get update
          sudo apt-get install -y chromium-browser chromium-chromedriver
          sudo apt-get install -y wget
          # Baixar e instalar o Google Chrome
          wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo dpkg -i google-chrome-stable_current_amd64.deb
          sudo apt-get install -f  # Corrigir dependências

      - name: Configurar Python e instalar dependências
        run: |
          python -m venv venv  # Cria um ambiente virtual
          source venv/bin/activate
          pip install --upgrade pip
          pip install -r requirements.txt  # Caso você tenha um arquivo requirements.txt
          # Ou instale pandas diretamente se não tiver um requirements.txt
          pip install pandas

      - name: Rodar o script de web scraping
        run: |
          source venv/bin/activate  # Ativa o ambiente virtual
          python webscrapping_dash_githubactions.py

      - name: Commitar e enviar o CSV atualizado
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "github-actions@github.com"
          git add nivel_dos_rios.csv
          git commit -m "Atualização automática do CSV"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
